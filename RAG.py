import json
from tqdm import tqdm  
from openai import OpenAI
from transformers import RagTokenizer, RagRetriever
from datasets import Dataset
import torch
import time

class RAGEnhancedProcessor:
    def __init__(self, knowledge_base_path, openai_api_key="0", openai_base_url="http://0.0.0.0:8000/v1"):
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.client = OpenAI(api_key=openai_api_key, base_url=openai_base_url)
        
        # åˆå§‹åŒ–æ—¶é—´ç»Ÿè®¡
        self.rag_init_time = 0
        self.rag_retrieval_times = []
        
        # è®°å½•åˆå§‹åŒ–å¼€å§‹æ—¶é—´
        init_start_time = time.time()
        
        # åŠ è½½çŸ¥è¯†åº“
        print("æ­£åœ¨åŠ è½½çŸ¥è¯†åº“...")
        kb_start = time.time()
        self.knowledge_base = self.load_knowledge_base(knowledge_base_path)
        kb_end = time.time()
        print(f"çŸ¥è¯†åº“åŠ è½½è€—æ—¶: {(kb_end - kb_start) * 1000:.1f}ms")
        
        # åˆå§‹åŒ–RAGç»„ä»¶
        print("æ­£åœ¨åŠ è½½RAGæ£€ç´¢å™¨...")
        rag_start = time.time()
        self.rag_tokenizer = RagTokenizer.from_pretrained("facebook/rag-token-base")
        
        # ä½¿ç”¨è‡ªå®šä¹‰çŸ¥è¯†åº“åˆ›å»ºæ£€ç´¢å™¨
        self.rag_retriever = self.create_custom_retriever()
        rag_end = time.time()
        
        self.rag_init_time = (rag_end - rag_start) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        print(f"RAGæ£€ç´¢å™¨åˆå§‹åŒ–è€—æ—¶: {self.rag_init_time:.1f}ms")
        
        total_init_time = (time.time() - init_start_time) * 1000
        print(f"æ€»åˆå§‹åŒ–è€—æ—¶: {total_init_time:.1f}ms")
        
    def load_knowledge_base(self, file_path):
        """åŠ è½½çŸ¥è¯†åº“æ–‡ä»¶"""
        try:
            if file_path.endswith('.json'):
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                # ç¡®ä¿è¿”å›çš„æ˜¯åˆ—è¡¨æ ¼å¼
                if isinstance(data, dict):
                    # å¦‚æœæ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
                    return [{"title": key, "text": value} for key, value in data.items()]
                elif isinstance(data, list):
                    return data
                else:
                    # å¦‚æœæ˜¯å…¶ä»–ç±»å‹ï¼Œè½¬æ¢ä¸ºå•é¡¹åˆ—è¡¨
                    return [{"title": "æ–‡æ¡£1", "text": str(data)}]
            elif file_path.endswith('.txt'):
                with open(file_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                # å°†æ–‡æœ¬æŒ‰æ®µè½åˆ†å‰²
                return [{"title": f"æ®µè½{i+1}", "text": line.strip()} for i, line in enumerate(lines) if line.strip()]
            else:
                raise ValueError("æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼š.json, .txt")
        except Exception as e:
            print(f"åŠ è½½çŸ¥è¯†åº“å¤±è´¥ï¼š{str(e)}")
            return []
    
    def create_custom_retriever(self):
        """åˆ›å»ºè‡ªå®šä¹‰æ£€ç´¢å™¨"""
        try:
            # å‡†å¤‡æ•°æ®é›†æ ¼å¼
            if self.knowledge_base and isinstance(self.knowledge_base, list):
                # ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®
                formatted_data = []
                for item in self.knowledge_base:
                    if isinstance(item, dict):
                        title = item.get('title', 'æ— æ ‡é¢˜')
                        text = item.get('text', item.get('content', ''))
                    else:
                        title = f"æ–‡æ¡£{len(formatted_data)+1}"
                        text = str(item)
                    
                    formatted_data.append({
                        "title": title,
                        "text": text
                    })
                
                # åˆ›å»ºæ•°æ®é›†
                dataset = Dataset.from_list(formatted_data)
                
                # è¿™é‡Œéœ€è¦æ„å»ºç´¢å¼•ï¼Œå®é™…ä½¿ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„é…ç½®
                print(f"çŸ¥è¯†åº“åŒ…å« {len(formatted_data)} ä¸ªæ–‡æ¡£")
                return None  # ç®€åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨æ–‡æœ¬åŒ¹é…
            else:
                print("çŸ¥è¯†åº“ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨é»˜è®¤æ£€ç´¢å™¨")
                return RagRetriever.from_pretrained("facebook/rag-token-base", index_name="exact", use_dummy_dataset=True)
                
        except Exception as e:
            print(f"åˆ›å»ºæ£€ç´¢å™¨å¤±è´¥ï¼š{str(e)}")
            return RagRetriever.from_pretrained("facebook/rag-token-base", index_name="exact", use_dummy_dataset=True)
    
    def simple_text_retrieval(self, query, top_k=3):
        """ç®€å•çš„æ–‡æœ¬åŒ¹é…æ£€ç´¢ï¼ˆå½“è‡ªå®šä¹‰æ£€ç´¢å™¨åˆ›å»ºå¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
        if not self.knowledge_base or not isinstance(self.knowledge_base, list):
            return ""
        
        try:
            import re
            query_words = set(re.findall(r'\w+', query.lower()))
            
            scored_docs = []
            for doc in self.knowledge_base:
                # ä¿®å¤ï¼šå®‰å…¨åœ°å¤„ç†ä¸åŒæ•°æ®ç±»å‹
                if isinstance(doc, dict):
                    title = doc.get('title', '')
                    text = doc.get('text', doc.get('content', ''))
                elif isinstance(doc, str):
                    title = f"æ–‡æ¡£{len(scored_docs)+1}"
                    text = doc
                else:
                    title = f"æ–‡æ¡£{len(scored_docs)+1}"
                    text = str(doc)
                
                # ç¡®ä¿titleå’Œtextéƒ½æ˜¯å­—ç¬¦ä¸²
                title = str(title) if title else ""
                text = str(text) if text else ""
                
                doc_text = f"{title} {text}".lower()
                doc_words = set(re.findall(r'\w+', doc_text))
                
                # è®¡ç®—è¯æ±‡é‡å åº¦
                overlap = len(query_words.intersection(doc_words))
                if overlap > 0:
                    # ç»Ÿä¸€æ ¼å¼åŒ–ä¸ºå­—å…¸
                    formatted_doc = {
                        'title': title,
                        'text': text
                    }
                    scored_docs.append((overlap, formatted_doc))
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›top_k
            scored_docs.sort(key=lambda x: x[0], reverse=True)
            top_docs = scored_docs[:top_k]
            
            context_parts = []
            for i, (score, doc) in enumerate(top_docs):
                title = doc.get('title', f'æ–‡æ¡£{i+1}')
                text = doc.get('text', '')
                context_parts.append(f"å‚è€ƒèµ„æ–™{i+1} - {title}:\n{text}")
            
            return "\n\n".join(context_parts)
            
        except Exception as e:
            print(f"æ–‡æœ¬æ£€ç´¢å¼‚å¸¸ï¼š{str(e)}")
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            print(f"çŸ¥è¯†åº“ç±»å‹: {type(self.knowledge_base)}")
            if self.knowledge_base and hasattr(self.knowledge_base, '__len__'):
                print(f"çŸ¥è¯†åº“é•¿åº¦: {len(self.knowledge_base)}")
                if len(self.knowledge_base) > 0:
                    print(f"ç¬¬ä¸€ä¸ªå…ƒç´ ç±»å‹: {type(self.knowledge_base[0])}")
                    print(f"ç¬¬ä¸€ä¸ªå…ƒç´ å†…å®¹: {str(self.knowledge_base[0])[:100]}...")
            return ""
    
    def retrieve_context(self, query, top_k=3, max_context_length=1000):
        """æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡"""
        # è®°å½•RAGæ£€ç´¢å¼€å§‹æ—¶é—´
        retrieval_start = time.time()
        
        # ä½¿ç”¨ç®€å•æ–‡æœ¬åŒ¹é…æ£€ç´¢
        context = self.simple_text_retrieval(query, top_k)
        
        # æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
        if len(context) > max_context_length:
            context = context[:max_context_length] + "..."
        
        # è®°å½•RAGæ£€ç´¢ç»“æŸæ—¶é—´
        retrieval_end = time.time()
        retrieval_time = (retrieval_end - retrieval_start) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        self.rag_retrieval_times.append(retrieval_time)
        
        return context, retrieval_time
    
    def create_enhanced_prompt(self, original_query, context, prompt_template=None):
        """åˆ›å»ºå¢å¼ºæç¤ºè¯"""
        if prompt_template is None:
            if context:
                prompt_template = """è¯·åŸºäºä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ï¼š

å‚è€ƒèµ„æ–™ï¼š
{context}

é—®é¢˜ï¼š{query}
"""
            else:
                return original_query
        
        return prompt_template.format(context=context, query=original_query)
    
    def process_single_case(self, case, retrieve_context=True, top_k=3, max_context_length=1000):
        """å¤„ç†å•ä¸ªæ¡ˆä¾‹"""
        original_input = case['input']
        context = ""
        rag_time = 0
        
        if retrieve_context:
            context, rag_time = self.retrieve_context(original_input, top_k, max_context_length)
        
        enhanced_prompt = self.create_enhanced_prompt(original_input, context)
        
        messages = [{"role": "user", "content": enhanced_prompt}]
        
        # è®°å½•APIè°ƒç”¨æ—¶é—´
        api_start = time.time()
        try:
            response = self.client.chat.completions.create(
                model="test",
                messages=messages
            )
            api_end = time.time()
            api_time = (api_end - api_start) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            result = {
                "predict": response.choices[0].message.content.strip(),
                "enhanced_prompt": enhanced_prompt,
                "retrieved_context": context,
                "context_length": len(context),
                "has_context": bool(context),
                "rag_retrieval_time_ms": rag_time,
                "api_call_time_ms": api_time,
                "total_time_ms": rag_time + api_time
            }
            
        except Exception as e:
            api_end = time.time()
            api_time = (api_end - api_start) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            print(f"\nå¤„ç†å¼‚å¸¸ï¼š{str(e)}")
            result = {
                "predict": "error",
                "enhanced_prompt": enhanced_prompt,
                "retrieved_context": context,
                "context_length": len(context),
                "has_context": bool(context),
                "rag_retrieval_time_ms": rag_time,
                "api_call_time_ms": api_time,
                "total_time_ms": rag_time + api_time,
                "error": str(e)
            }
        
        return result
    
    def get_rag_statistics(self):
        """è·å–RAGæ—¶é—´ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ¯«ç§’ï¼‰"""
        if not self.rag_retrieval_times:
            return {
                "total_retrievals": 0,
                "total_retrieval_time_ms": 0,
                "avg_retrieval_time_ms": 0,
                "min_retrieval_time_ms": 0,
                "max_retrieval_time_ms": 0,
                "init_time_ms": self.rag_init_time
            }
        
        return {
            "total_retrievals": len(self.rag_retrieval_times),
            "total_retrieval_time_ms": sum(self.rag_retrieval_times),
            "avg_retrieval_time_ms": sum(self.rag_retrieval_times) / len(self.rag_retrieval_times),
            "min_retrieval_time_ms": min(self.rag_retrieval_times),
            "max_retrieval_time_ms": max(self.rag_retrieval_times),
            "init_time_ms": self.rag_init_time
        }
    
    def debug_knowledge_base(self):
        """è°ƒè¯•çŸ¥è¯†åº“æ ¼å¼"""
        print(f"\nğŸ” çŸ¥è¯†åº“è°ƒè¯•ä¿¡æ¯:")
        print(f"  çŸ¥è¯†åº“ç±»å‹: {type(self.knowledge_base)}")
        
        if self.knowledge_base is None:
            print("  çŸ¥è¯†åº“ä¸ºç©º(None)")
            return
            
        if hasattr(self.knowledge_base, '__len__'):
            print(f"  çŸ¥è¯†åº“é•¿åº¦: {len(self.knowledge_base)}")
        else:
            print("  çŸ¥è¯†åº“æ²¡æœ‰é•¿åº¦å±æ€§")
            return
        
        if isinstance(self.knowledge_base, list) and self.knowledge_base:
            # æ˜¾ç¤ºå‰3ä¸ªå…ƒç´ 
            for i in range(min(3, len(self.knowledge_base))):
                item = self.knowledge_base[i]
                print(f"  é¡¹ç›® {i+1} ç±»å‹: {type(item)}")
                print(f"  é¡¹ç›® {i+1} å†…å®¹: {str(item)[:100]}...")
                if isinstance(item, dict):
                    print(f"  é¡¹ç›® {i+1} é”®: {list(item.keys())}")
        elif isinstance(self.knowledge_base, dict):
            print("  çŸ¥è¯†åº“æ˜¯å­—å…¸æ ¼å¼ï¼Œé”®:")
            keys = list(self.knowledge_base.keys())
            for i, key in enumerate(keys[:3]):
                print(f"    é”® {i+1}: {key}")
        else:
            print(f"  çŸ¥è¯†åº“æ˜¯ {type(self.knowledge_base)} ç±»å‹")

# ä½¿ç”¨ç¤ºä¾‹ - åœ¨è¿™é‡ŒæŒ‡å®šä½ çš„çŸ¥è¯†åº“æ–‡ä»¶è·¯å¾„
KNOWLEDGE_BASE_PATH = "/home/lab1015/programmes/sjc/LLM/RAG2.json"  # ä¿®æ”¹ä¸ºä½ çš„çŸ¥è¯†åº“è·¯å¾„

# è®°å½•æ•´ä½“å¼€å§‹æ—¶é—´
total_start_time = time.time()

processor = RAGEnhancedProcessor(KNOWLEDGE_BASE_PATH)

# æ·»åŠ è°ƒè¯•ä¿¡æ¯
processor.debug_knowledge_base()

with open('/home/lab1015/programmes/sjc/LLM/ragqwen.json', 'r+', encoding='utf-8') as f:
    test_data = json.load(f)
    
    print(f"\nå¼€å§‹å¤„ç† {len(test_data)} ä¸ªæ¡ˆä¾‹...")
    processing_start_time = time.time()
    
    for index, case in enumerate(tqdm(test_data, desc='å¤„ç†è¿›åº¦', unit='æ¡')):
        result = processor.process_single_case(
            case, 
            retrieve_context=True,
            top_k=3,
            max_context_length=800
        )
        
        test_data[index].update(result)

    processing_end_time = time.time()
    
    f.seek(0)
    json.dump(test_data, f, indent=2, ensure_ascii=False)
    f.truncate()

total_end_time = time.time()

# è¾“å‡ºè¯¦ç»†çš„æ—¶é—´ç»Ÿè®¡
print("\n" + "="*50)
print("å¤„ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜")
print("="*50)

# RAGç»Ÿè®¡ä¿¡æ¯
rag_stats = processor.get_rag_statistics()
print(f"\nğŸ“Š RAGæ¨¡å—æ—¶é—´ç»Ÿè®¡:")
print(f"  åˆå§‹åŒ–æ—¶é—´: {rag_stats['init_time_ms']:.1f}ms")
print(f"  æ£€ç´¢æ¬¡æ•°: {rag_stats['total_retrievals']}")
print(f"  æ€»æ£€ç´¢æ—¶é—´: {rag_stats['total_retrieval_time_ms']:.1f}ms")
print(f"  å¹³å‡æ£€ç´¢æ—¶é—´: {rag_stats['avg_retrieval_time_ms']:.1f}ms")
print(f"  æœ€å¿«æ£€ç´¢æ—¶é—´: {rag_stats['min_retrieval_time_ms']:.1f}ms")
print(f"  æœ€æ…¢æ£€ç´¢æ—¶é—´: {rag_stats['max_retrieval_time_ms']:.1f}ms")

# æ•´ä½“ç»Ÿè®¡
total_processing_time = (processing_end_time - processing_start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
total_program_time = (total_end_time - total_start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’

print(f"\nâ±ï¸  æ•´ä½“æ—¶é—´ç»Ÿè®¡:")
print(f"  æ•°æ®å¤„ç†æ—¶é—´: {total_processing_time:.1f}ms")
print(f"  ç¨‹åºæ€»è¿è¡Œæ—¶é—´: {total_program_time:.1f}ms")
print(f"  å¹³å‡æ¯ä¸ªæ¡ˆä¾‹å¤„ç†æ—¶é—´: {total_processing_time/len(test_data):.1f}ms")

# è®¡ç®—APIè°ƒç”¨æ—¶é—´ç»Ÿè®¡
api_times = [case.get('api_call_time_ms', 0) for case in test_data if 'api_call_time_ms' in case]
if api_times:
    print(f"\nğŸŒ APIè°ƒç”¨æ—¶é—´ç»Ÿè®¡:")
    print(f"  æ€»APIè°ƒç”¨æ—¶é—´: {sum(api_times):.1f}ms")
    print(f"  å¹³å‡APIè°ƒç”¨æ—¶é—´: {sum(api_times)/len(api_times):.1f}ms")
    print(f"  æœ€å¿«APIè°ƒç”¨: {min(api_times):.1f}ms")
    print(f"  æœ€æ…¢APIè°ƒç”¨: {max(api_times):.1f}ms")

# è®¡ç®—æˆåŠŸç‡
successful_cases = sum(1 for case in test_data if case.get('predict', '') != 'error')
success_rate = successful_cases / len(test_data) * 100
cases_with_context = sum(1 for case in test_data if case.get('has_context', False))
context_rate = cases_with_context / len(test_data) * 100

print(f"\nğŸ“ˆ å¤„ç†ç»“æœç»Ÿè®¡:")
print(f"  æ€»å¤„ç†æ¡ˆä¾‹: {len(test_data)}")
print(f"  æˆåŠŸå¤„ç†æ¡ˆä¾‹: {successful_cases}")
print(f"  æˆåŠŸç‡: {success_rate:.1f}%")
print(f"  åŒ…å«æ£€ç´¢ä¸Šä¸‹æ–‡çš„æ¡ˆä¾‹: {cases_with_context}")
print(f"  ä¸Šä¸‹æ–‡è¦†ç›–ç‡: {context_rate:.1f}%")

# RAGä¸APIæ—¶é—´å¯¹æ¯”
if api_times and rag_stats['total_retrievals'] > 0:
    print(f"\nğŸ” RAG vs APIæ—¶é—´å¯¹æ¯”:")
    print(f"  RAGæ£€ç´¢å¹³å‡æ—¶é—´: {rag_stats['avg_retrieval_time_ms']:.1f}ms")
    print(f"  APIè°ƒç”¨å¹³å‡æ—¶é—´: {sum(api_times)/len(api_times):.1f}ms")
    ratio = (sum(api_times)/len(api_times)) / rag_stats['avg_retrieval_time_ms']
    print(f"  API/RAGæ—¶é—´æ¯”ä¾‹: {ratio:.1f}x")

print("\n" + "="*50)